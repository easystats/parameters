---
title: "Parameters standardization"
output: 
  github_document:
    toc: true
    fig_width: 10.08
    fig_height: 6
  rmarkdown::html_vignette:
    toc: true
    fig_width: 10.08
    fig_height: 6
tags: [r, parameters]
vignette: >
  %\VignetteIndexEntry{Get Started with Bayesian Analysis}
  \usepackage[utf8]{inputenc}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
bibliography: bibliography.bib
---

# Introduction

Standardising parameters (*i.e.*, coefficients) can allow for their comparison within and between models, variables and studies. Moreover, as it returns coefficients expressed in terms of variance modification (for instance, coefficients expresed in terms of SD of the response variable), it can allow for the usage of [effect size interpretation guidelines](https://easystats.github.io/report/articles/interpret_metrics.html), such as the famous Cohen's (1988) rules of thumb.

However, standardizing the model's parameters should not be automatic: for some research fields, particular variables or types of studies (*e.g.*, replications), it sometimes makes more sense to keep, use and interpret the original parameters, especially if they are well known or easily understood.

Critically, parameters standardization is not a **trivial process**. Different techniques exist, that can lead to drastically different results. Thus, it is critical that the standardization method is explicitly documented and detailed.

The `parameters` package include the possibility of using different techniques, described below [@bring1994standardize;@menard2004six;@gelman2008scaling;@schielzeth2010simple;@menard2011standards].


# Standardization methods

## `refit`: Re-fitting the model with standardized data

This method is based on a complete model re-fit with a standardized version of data. Hence, this method is equal to standardizing the variables before fitting the model. It is the "purest" and the most accurate [@neter1989applied], but it is also the most computationally costly and long (especially for Bayesian models). This method is particularly recommended for complex models that include interactions or transformations (*e.g.*, polynomial or spline terms).


```{r message=FALSE, warning=FALSE, results='hide'}
library(parameters)

data <- iris
model <- lm(Sepal.Length ~ Petal.Width + Sepal.Width, data=data)

standardize_parameters(model, method="refit")
```
```{r message=FALSE, warning=FALSE, echo=FALSE}
knitr::kable(standardize_parameters(model, method="refit"), digits=2)
```

The `robust` (default to `FALSE`) argument enables a **robust standardization of data**, *i.e.*, based on the **median** and **MAD** instead of the **mean** and **SD**.

```{r warning=FALSE, message=FALSE, results='hide'}
standardize_parameters(model, method="refit", robust=TRUE)
```
```{r message=FALSE, warning=FALSE, echo=FALSE}
knitr::kable(standardize_parameters(model, method="refit", robust=TRUE), digits=2)
```


This method is very flexible as it can be applied to all types of models (linear, logistic...).

```{r warning=FALSE, message=FALSE, results='hide'}
library(parameters)

data$binary <- ifelse(data$Sepal.Width > 3, 1, 0)
model <- glm(binary ~ Species + Sepal.Length, data = data, family="binomial")
standardize_parameters(model, method="refit")
```
```{r message=FALSE, warning=FALSE, echo=FALSE}
knitr::kable(standardize_parameters(model, method="refit"), digits=2)
```

## `2sd`: Scaling by two 2 SDs

Same as `method = "refit"`, however, standardization is done by dividing by two times the `SD` or `MAD` (depending on `robust`). This method is useful to obtain coefficients of continuous parameters comparable to coefficients related to binary predictors (see @gelman2008scaling).



## `full`: Scaling by the variances of the response and the predictor

Post-hoc standardization of the model paramaters. The coefficients are divided by the standard deviation (or MAD if `robust`) of the outcome (which becomes their expression 'unit'). Then, the coefficients related to numeric variables are additionaly multiplied by the standard deviation (or MAD if `robust`) of the related term, so that they correspond to changes of 1 SD of the predictor (e.g., "A change in 1 SD of `x` is related to a change of 0.24 of the SD of `y`). This does not apply to binary variables or factors, so the coefficients are still related to changes in levels.


```{r warning=FALSE, message=FALSE, results='hide'}
model <- lm(Sepal.Length ~ Petal.Width + Sepal.Width, data=data)
standardize_parameters(model, method="full")
```
```{r message=FALSE, warning=FALSE, echo=FALSE}
knitr::kable(standardize_parameters(model, method="full"), digits=2)
```

## `classic`: Basic scaling of all parameters

This method is similar to `method = "full"`, but treats all variables as continuous: it also scales the coefficient by the standard deviation of factors (transformed to integers) or binary predictors. Altough being inapropriate for these cases, this method is the one implemented by default in other softwares, such as `sjstats::std_beta()` or `lm.beta::lm.beta()`.

# Methods Comparison

We will use the "refit" method as the baseline. We will then compute the differences between these standardized parameters and the ones provided by the other functions. The **bigger the (absolute) number, the worse it is**.

```{r message=FALSE, warning=FALSE}
library(parameters)
library(sjstats)
library(lm.beta)

comparison <- function(model, robust=FALSE){
  out <- standardize_parameters(model, method="refit", robust=robust)[1:2]
  
  out$full <- tryCatch({
    out[, 2] - standardize_parameters(model, method="full", robust=robust)[, 2]
}, error = function(error_condition) {
    "Error"
})
  out$classic <- tryCatch({
    out[, 2] - standardize_parameters(model, method="classic", robust=robust)[, 2]
}, error = function(error_condition) {
    "Error"
})
  out$sjstats <- tryCatch({
    if(insight::model_info(model)$is_bayesian){
      out[, 2] - c(NA, parameters::describe_posterior(sjstats::std_beta(model))[, 1])
    } else{
      out[, 2] - c(NA, sjstats::std_beta(model)[, 2])
    }
}, error = function(error_condition) {
    "Error"
})
  
  out$lm.beta <- tryCatch({
    out[, 2] - lm.beta::lm.beta(model)$standardized.coefficients
}, error = function(error_condition) {
    "Error"
})
  
#   out$MuMin <- tryCatch({
#     out$Std_Estimate - MuMIn::std.coef(model, partial.sd=FALSE)[, 1]
# }, error = function(error_condition) {
#     "Error"
# })

  out[, 2] <- NULL
  return(out)
}
```


## Models with only numeric predictors


### Linear Model


```{r message=FALSE, warning=FALSE, results='hide'}
data <- iris
data$Group_Sepal.Width <- as.factor(ifelse(data$Sepal.Width > 3, "High", "Low"))
data$Binary_Sepal.Width <- as.factor(ifelse(data$Sepal.Width > 3, 1, 0))

model <- lm(Sepal.Length ~ Petal.Width + Sepal.Width, data=data) 
comparison(model)
```
```{r message=FALSE, warning=FALSE, echo=FALSE}
knitr::kable(comparison(model), digits=2)
```

For this simple model, **all methods return results equal to the "refit" method**.

### Logistic Model


```{r message=FALSE, warning=FALSE, results='hide'}
model <- glm(Binary_Sepal.Width ~ Petal.Width + Sepal.Length, data=data, family="binomial")
comparison(model)
```
```{r message=FALSE, warning=FALSE, echo=FALSE}
knitr::kable(comparison(model), digits=2)
```


### Linear Mixed Model


```{r message=FALSE, warning=FALSE, results='hide'}
library(lme4)

model <- lme4::lmer(Sepal.Length ~ Petal.Width + Sepal.Width + (1|Species), data=data)
comparison(model)
```
```{r message=FALSE, warning=FALSE, echo=FALSE}
knitr::kable(comparison(model), digits=2)
```

For this simple mixed model, **all methods return results equal to the "refit" method**.

### Interactions

```{r message=FALSE, warning=FALSE, results='hide'}
model <- lm(Sepal.Length ~ Petal.Width * Sepal.Width, data=data)
comparison(model)
```
```{r message=FALSE, warning=FALSE, echo=FALSE}
knitr::kable(comparison(model), digits=2)
```


When interactions are involved, post-hoc methods return different results. However, methods implemented in other softwares perform arguably worse.

<!-- haha Sorry Daniel maybe we should rephrase this ^^ -->

### Transformation

```{r message=FALSE, warning=FALSE, results='hide'}
model <- lm(Sepal.Length ~ poly(Petal.Width, 2) + poly(Sepal.Width, 2), data=data)
comparison(model)
```
```{r message=FALSE, warning=FALSE, echo=FALSE}
knitr::kable(comparison(model), digits=2)
```

For polynomial transformations, other software become very unreliable.

### Bayesian Models

```{r message=FALSE, warning=FALSE, eval=FALSE}
library(rstanarm)

model <- stan_glm(Sepal.Length ~ Petal.Width + Sepal.Width, data=data)
comparison(model)
```
```{r message=FALSE, warning=FALSE, echo=FALSE}
library(rstanarm)
junk <- capture.output(model <- stan_glm(Sepal.Length ~ Petal.Width + Sepal.Width, data=data))
knitr::kable(comparison(model), digits=2)
```



## Models with factors

### Linear Model


```{r message=FALSE, warning=FALSE, results='hide'}
model <- lm(Sepal.Length ~ Petal.Width + Group_Sepal.Width, data=data) 
comparison(model)
```
```{r message=FALSE, warning=FALSE, echo=FALSE}
knitr::kable(comparison(model), digits=2)
```


### Logistic Model


```{r message=FALSE, warning=FALSE, results='hide'}
model <- glm(Binary_Sepal.Width ~ Petal.Width + Species, data=data, family="binomial")
comparison(model)
```
```{r message=FALSE, warning=FALSE, echo=FALSE}
knitr::kable(comparison(model), digits=2)
```


### Linear Mixed Model


```{r message=FALSE, warning=FALSE, results='hide'}
library(lme4)

model <- lme4::lmer(Sepal.Length ~ Petal.Length + Group_Sepal.Width + (1|Species), data=data)
comparison(model)
```
```{r message=FALSE, warning=FALSE, echo=FALSE}
knitr::kable(comparison(model), digits=2)
```

### Interactions

```{r message=FALSE, warning=FALSE, results='hide'}
model <- lm(Sepal.Length ~ Petal.Width * Group_Sepal.Width, data=data)
comparison(model)
```
```{r message=FALSE, warning=FALSE, echo=FALSE}
knitr::kable(comparison(model), digits=2)
```



### Bayesian Models

```{r message=FALSE, warning=FALSE, eval=FALSE}
library(rstanarm)

model <- stan_glm(Sepal.Length ~ Petal.Width + Group_Sepal.Width, data=data)
comparison(model)
```
```{r message=FALSE, warning=FALSE, echo=FALSE}
library(rstanarm)
junk <- capture.output(model <- stan_glm(Sepal.Length ~ Petal.Width + Group_Sepal.Width, data=data))
knitr::kable(comparison(model), digits=2)
```


# Conclusion

Use `refit` if possible, otherwise `full`.


# References




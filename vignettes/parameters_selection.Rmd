---
title: "Selection of Model Parameters"
output: 
  rmarkdown::html_vignette:
    toc: true
    fig_width: 10.08
    fig_height: 6
tags: [r, parameters, variable selection, feature selection]
vignette: >
  %\VignetteIndexEntry{Selection of Model Parameters}
  \usepackage[utf8]{inputenc}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
bibliography: bibliography.bib
---

```{r , include=FALSE}
library(knitr)
options(knitr.kable.NA = "")
options(digits = 2)

knitr::opts_chunk$set(
  echo = TRUE,
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  comment = "#>",
  out.width = "100%"
)

pkgs <- c("performance", "cAIC4", "lme4")
successfully_loaded <- vapply(pkgs, requireNamespace, TRUE, quietly = TRUE)

if (all(successfully_loaded)) {
  library(parameters)
  library(lme4)
}

set.seed(333)
```

Also known as [**feature selection**](https://en.wikipedia.org/wiki/Feature_selection) in machine
learning, the goal of variable selection is to **identify a subset of predictors** to **simplify models**. This can benefit model interpretation,
shorten fitting time, and improve generalization (by reducing overfitting).

There are many different methods. The appropriate method for a given problem
will depend on the model type, the data, the objective, and the theoretical
rationale.

The `parameters` package implements a helper that will **automatically** pick a
method deemed appropriate for the provided model, run the variables selection
and return the **optimal formula**, which you can then re-use to update the
model.

## Simple linear regression

### Fit a powerful model

If you are familiar with R and the formula interface, you know of the
possibility of including a dot (`.`) in the formula, signifying "all the
remaining variables". Curiously, few are aware of the possibility of
additionally easily adding "all the interaction terms". This can be achieved
using the `.*.` notation.

Let's try that with the linear regression predicting **Sepal.Length** with the
[`iris`](https://en.wikipedia.org/wiki/Iris_flower_data_set) dataset, included
by default in R.

```{r}
model <- lm(Sepal.Length ~ . * ., data = iris)
summary(model)
```

***Wow, that's a lot of parameters! And almost none of them are significant!***

Which is ***weird***, considering that **gorgeous $R^2$ of 0.882!** 

*I wish I had that in my research!*

### Too many parameters?

As you might know, having a **model that is too performant is not always a good
thing**. For instance, it can be a marker of
[**overfitting**](https://en.wikipedia.org/wiki/Overfitting): the model
corresponds too closely to a particular set of data, and may therefore fail to
predict future observations reliably. In multiple regressions, in can also fall
under the [**Freedman's paradox**](https://en.wikipedia.org/wiki/Freedman%27s_paradox): some predictors
that have actually no relation to the dependent variable being predicted will be
**spuriously found to be statistically significant**.

Let's run a few checks using the
[**performance**](https://github.com/easystats/performance) package:

```{r eval=successfully_loaded["performance"]}
library(performance)

check_normality(model)
check_heteroscedasticity(model)
check_autocorrelation(model)
check_collinearity(model)
```

The main issue of the model seems to be the high
[multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity). This
suggests that our model might not be able to give valid results about any
individual predictor, nor tell which predictors are redundant with respect to
others.

### Parameters selection

Time to do some variables selection! This can be easily done using the
`select_parameters()` function in `parameters`. It will **automatically** select
the best variables and update the model accordingly. One way of using that is in
a tidy pipeline (using [`%>%`](https://cran.r-project.org/package=magrittr/)),
using this output to update a new model.

```{r}
library(parameters)
lm(Sepal.Length ~ . * ., data = iris) |>
  select_parameters() |>
  summary()
```

That's still a lot of parameters, but as you can see, almost all of them are
now significant, and the $R^2$ did not change much.

Although appealing, please note that these automated selection methods are
[**quite criticized**](https://towardsdatascience.com/stopping-stepwise-why-stepwise-selection-is-bad-and-what-you-should-use-instead-90818b3f52df),
and should not be used in place of **theoretical** or **hypothetical** reasons
(*i.e.*, you should have *a priori* hypotheses about which parameters of your
model you want to focus on).

## Mixed models

For simple linear regressions as above, the selection is made using the `step()`
function (available in base R). This performs a
[**stepwise**](https://en.wikipedia.org/wiki/Stepwise_regression) selection.
However, this procedures is not available for other types of models, such as
**mixed** models.

### Mixed models

For mixed models (of class `merMod`), stepwise selection is based on
`cAIC4::stepcAIC()`. This step function only searches the "best" model based on
the _random effects structure_, i.e. `select_parameters()` adds or excludes
random effects until the `cAIC` can't be improved further.

This is what our initial model looks like.

```{r eval=successfully_loaded["lme4"]}
library(lme4)
data("qol_cancer")

# initial model
lmer(
  QoL ~ time + phq4 + age + (1 + time | hospital / ID),
  data = qol_cancer
) |>
  summary()
```

This is the model selected by `select_parameters()`. Please notice the
differences in the random effects structure between the initial and the selected
models:

```{r, eval=FALSE}
## TODO: this is currently broken due to an issue in package cAIC4

# multiple models are checked, however, initial models
# already seems to be the best one...
lmer(
  QoL ~ time + phq4 + age + (1 + time | hospital / ID),
  data = qol_cancer
)  |>
  select_parameters() |>
  summary()
```

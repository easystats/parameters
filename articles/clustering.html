<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="parameters">
<title>Clustering with easystats • parameters</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.2.2/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.2.2/bootstrap.bundle.min.js"></script><link href="../deps/Roboto-0.4.4/font.css" rel="stylesheet">
<link href="../deps/JetBrains_Mono-0.4.4/font.css" rel="stylesheet">
<link href="../deps/Roboto_Slab-0.4.4/font.css" rel="stylesheet">
<!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Clustering with easystats">
<meta property="og:description" content="parameters">
<meta property="og:image" content="https://easystats.github.io/parameters/reference/figures/card.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:creator" content="@easystats4u">
<meta name="twitter:site" content="@easystats4u">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">parameters</a>

    <small class="nav-text text-default me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.20.0.7</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html" aria-label="Reference">
    <span class="fa fa fa fa-file-code"></span>
     
    Reference
  </a>
</li>
<li class="active nav-item">
  <a class="nav-link" href="../articles/index.html" aria-label="Articles">
    <span class="fa fa fa fa-book-reader"></span>
     
    Articles
  </a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../news/index.html" aria-label="News">
    <span class="fa fa fa fa-newspaper"></span>
     
    News
  </a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown--easystats" aria-label="easystats">
    <span class="fab fa fab fa-r-project"></span>
     
    easystats
  </a>
  <div class="dropdown-menu" aria-labelledby="dropdown--easystats">
    <a class="external-link dropdown-item" href="https://easystats.github.io/bayestestR/">bayestestR</a>
    <a class="external-link dropdown-item" href="https://easystats.github.io/correlation/">correlation</a>
    <a class="external-link dropdown-item" href="https://easystats.github.io/datawizard/">datawizard</a>
    <a class="external-link dropdown-item" href="https://easystats.github.io/easystats/">easystats</a>
    <a class="external-link dropdown-item" href="https://easystats.github.io/effectsize/">effectsize</a>
    <a class="external-link dropdown-item" href="https://easystats.github.io/insight/">insight</a>
    <a class="external-link dropdown-item" href="https://easystats.github.io/modelbased/">modelbased</a>
    <a class="external-link dropdown-item" href="https://easystats.github.io/performance/">performance</a>
    <a class="dropdown-item" href="https://easystats.github.io/parameters/">parameters</a>
    <a class="external-link dropdown-item" href="https://easystats.github.io/report/">report</a>
    <a class="external-link dropdown-item" href="https://easystats.github.io/see/">see</a>
  </div>
</li>
<li class="nav-item">
  <a class="external-link nav-link" href="http://twitter.com/easystats4u" aria-label="Twitter">
    <span class="fa fa-twitter"></span>
     
  </a>
</li>
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/easystats/parameters/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>Clustering with easystats</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/easystats/parameters/blob/HEAD/vignettes/clustering.Rmd" class="external-link"><code>vignettes/clustering.Rmd</code></a></small>
      <div class="d-none name"><code>clustering.Rmd</code></div>
    </div>

    
    
<p>This vignette can be referred to by citing the package:</p>
<ul>
<li>Lüdecke, D., Ben-Shachar, M. S., Patil, I., &amp; Makowski, D.
(2020). <em>Extracting, computing and exploring the parameters of
statistical models using R</em>. Journal of Open Source Software, 5(53),
2445. <a href="https://doi.org/10.21105/joss.02445" class="external-link uri">https://doi.org/10.21105/joss.02445</a>
</li>
</ul>
<p>Note that in order to fully use all the methods demonstrated below,
you will need to additionally install the packages below:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/install.packages.html" class="external-link">install.packages</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"NbClust"</span>, <span class="st">"mclust"</span>, <span class="st">"pvclust"</span>, <span class="st">"cluster"</span>, <span class="st">"fpc"</span>, <span class="st">"dbscan"</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>Clustering traditionally refers to the identification of groups of
observations (i.e., data rows). It differs from methods like <a href="https://easystats.github.io/parameters/articles/efa_cfa.html"><strong>PCA
or Factor Analysis</strong></a>, which are usually applied on variables
(i.e., columns). That said, it is possible to <em>transpose</em> your
data (columns become rows) to apply clustering on variables.</p>
<p>There are many clustering algorithms (see <a href="https://scikit-learn.org/stable/modules/clustering.html" class="external-link">this for
an overview</a>), but they can grouped in two categories:
<strong>supervised</strong> and <strong>unsupervised</strong>
techniques. In <strong>supervised</strong> techniques, you have to
explicitly specify <a href="https://easystats.github.io/parameters/reference/n_clusters.html"><strong>how
many clusters</strong></a> you want to extract.
<strong>Unsupervised</strong> techniques, on the other hand, will
estimate this number as part of their algorithm. Note that there are no
inherently superior and inferior clustering methods, each come with
their sets of limitations and benefits.</p>
<p>As an example in the tutorial below, we will use the
<strong>iris</strong> dataset, for which we know that there are 3 “real”
clusters (the 3 Species of flowers). Let’s first start with visualizing
the 3 “real” clusters on a 2D space of the variables created through
PCA.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org" class="external-link">ggplot2</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://easystats.github.io/parameters/">parameters</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://easystats.github.io/see/" class="external-link">see</a></span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">33</span><span class="op">)</span> <span class="co"># Set random seed</span></span>
<span></span>
<span><span class="co"># Select the first 4 numeric columns (drop the Species fator)</span></span>
<span><span class="va">data</span> <span class="op">&lt;-</span> <span class="va">iris</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">4</span><span class="op">]</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">data</span><span class="op">)</span> <span class="co"># Print the 6 first rows</span></span>
<span><span class="co">#&gt;   Sepal.Length Sepal.Width Petal.Length Petal.Width</span></span>
<span><span class="co">#&gt; 1          5.1         3.5          1.4         0.2</span></span>
<span><span class="co">#&gt; 2          4.9         3.0          1.4         0.2</span></span>
<span><span class="co">#&gt; 3          4.7         3.2          1.3         0.2</span></span>
<span><span class="co">#&gt; 4          4.6         3.1          1.5         0.2</span></span>
<span><span class="co">#&gt; 5          5.0         3.6          1.4         0.2</span></span>
<span><span class="co">#&gt; 6          5.4         3.9          1.7         0.4</span></span>
<span></span>
<span><span class="co"># Run PCA</span></span>
<span><span class="va">pca</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/principal_components.html">principal_components</a></span><span class="op">(</span><span class="va">data</span>, n <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">pca_scores</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">pca</span>, names <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"PCA_1"</span>, <span class="st">"PCA_2"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">pca_scores</span><span class="op">$</span><span class="va">True_Clusters</span> <span class="op">&lt;-</span> <span class="va">iris</span><span class="op">$</span><span class="va">Species</span> <span class="co"># Add real clusters</span></span>
<span></span>
<span><span class="co"># Visualize</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html" class="external-link">ggplot</a></span><span class="op">(</span><span class="va">pca_scores</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html" class="external-link">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">PCA_1</span>, y <span class="op">=</span> <span class="va">PCA_2</span>, color <span class="op">=</span> <span class="va">True_Clusters</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html" class="external-link">geom_point</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://easystats.github.io/see/reference/theme_modern.html" class="external-link">theme_modern</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<p><img src="clustering_files/figure-html/unnamed-chunk-3-1.png" width="700"></p>
<p>While the <strong>setosa</strong> species stands out quite clearly in
this PCA space, the separation between the two other species appear less
clear cut. Let’s see how data-driven clustering performs, and if we
manage to retrieve these 3 clusters.</p>
</div>
<div class="section level2">
<h2 id="supervised-clustering-methods">Supervised Clustering Methods<a class="anchor" aria-label="anchor" href="#supervised-clustering-methods"></a>
</h2>
<div class="section level3">
<h3 id="how-many-clusters-to-extract">How Many Clusters to Extract?<a class="anchor" aria-label="anchor" href="#how-many-clusters-to-extract"></a>
</h3>
<p>There is no easy answer to that important question. The best way is
to have strong expectations or hypotheses. If you don’t, well,
researchers have came up with data-driven solutions to estimate the
optimal number of clusters. The problem is that there are now a lot of
these numerical methods, and that they don’t always agree…</p>
<p>Because there is no clearly better method, we have implemented in
<em>easystats</em> a consensus-based algorithm that runs many of these
methods, and returns the number of clusters that is the most agreed
upon.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/n_clusters.html">n_clusters</a></span><span class="op">(</span><span class="va">data</span>, package <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"easystats"</span>, <span class="st">"NbClust"</span>, <span class="st">"mclust"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">n</span></span>
<span><span class="co">#&gt; # Method Agreement Procedure:</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; The choice of 2 clusters is supported by 15 (51.72%) methods out of 29 (Elbow, Silhouette, Gap_Maechler2012, Gap_Dudoit2002, Ch, DB, Duda, Pseudot2, Beale, Ratkowsky, PtBiserial, Mcclain, Dunn, SDindex, Mixture (VVV)).</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">n</span><span class="op">)</span></span></code></pre></div>
<p><img src="clustering_files/figure-html/unnamed-chunk-4-1.png" width="700"></p>
<p>As we can see, most methods suggest the existence of <strong>2
clusters</strong>, followed by a <strong>3-clusters</strong> solution.
It seems like the data does not clearly discriminate between the 3
species of flowers. This discrepancy between what is, and what we can
recover from real-world data, is a fundamental issue in data
science.</p>
</div>
<div class="section level3">
<h3 id="k-means">K-Means<a class="anchor" aria-label="anchor" href="#k-means"></a>
</h3>
<p>We won’t go too much into details about the mathematics and intuition
behind these clustering methods, as good <a href="https://scikit-learn.org/stable/modules/clustering.html" class="external-link">resources</a>
are available all over the internet. Instead, we’ll focus on how to
apply them.</p>
<p>K-means is one of the most basic clustering algorithm, available in
base R through the <code><a href="https://rdrr.io/r/stats/kmeans.html" class="external-link">kmeans()</a></code> function. However, we provide
in easystats a unified function to run different clustering algorithms:
<a href="https://easystats.github.io/parameters/reference/cluster_analysis.html"><strong>cluster_analysis()</strong></a>.
<em>(Note that k-means is a non-deterministic algorithm; running it
multiple times will result in different results!)</em></p>
<p>Now that we know how many clusters we want to extract (let’s say that
we have a strong hypothesis on 3, which is partially supported by the
consensus method for estimating the optimal number of clusters).</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">rez_kmeans</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cluster_analysis.html">cluster_analysis</a></span><span class="op">(</span><span class="va">data</span>, n <span class="op">=</span> <span class="fl">3</span>, method <span class="op">=</span> <span class="st">"kmeans"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">rez_kmeans</span> <span class="co"># Show results</span></span>
<span><span class="co">#&gt; # Clustering Solution</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; The 3 clusters accounted for 76.70% of the total variance of the original data.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Cluster | n_Obs | Sum_Squares | Sepal.Length | Sepal.Width | Petal.Length | Petal.Width</span></span>
<span><span class="co">#&gt; ---------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 1       |    53 |       44.09 |        -0.05 |       -0.88 |         0.35 |        0.28</span></span>
<span><span class="co">#&gt; 2       |    47 |       47.45 |         1.13 |        0.09 |         0.99 |        1.01</span></span>
<span><span class="co">#&gt; 3       |    50 |       47.35 |        -1.01 |        0.85 |        -1.30 |       -1.25</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; # Indices of model performance</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Sum_Squares_Total | Sum_Squares_Between | Sum_Squares_Within |    R2</span></span>
<span><span class="co">#&gt; --------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 596.000           |             457.112 |            138.888 | 0.767</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; # You can access the predicted clusters via `predict()`.</span></span></code></pre></div>
<p>Note that we can also visualize the <strong>centers</strong> (i.e.,
the “average” of each variable for each cluster):</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">rez_kmeans</span><span class="op">)</span><span class="op">)</span> <span class="co"># Visualize cluster centers</span></span></code></pre></div>
<p><img src="clustering_files/figure-html/unnamed-chunk-6-1.png" width="700"></p>
<p>One can extract the cluster assignments to use it as a new variable
by using <code><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict()</a></code>.</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">rez_kmeans</span><span class="op">)</span> <span class="co"># Get clusters</span></span>
<span><span class="co">#&gt;   [1] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3</span></span>
<span><span class="co">#&gt;  [38] 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 1 1 1 2 1 1 1 1 1 1 1 1 2 1 1 1 1 2 1 1 1</span></span>
<span><span class="co">#&gt;  [75] 1 2 2 2 1 1 1 1 1 1 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 2 2 2 1 2 2 2 2</span></span>
<span><span class="co">#&gt; [112] 2 2 1 1 2 2 2 2 1 2 1 2 1 2 2 1 2 2 2 2 2 2 1 1 2 2 2 1 2 2 2 1 2 2 2 1 2</span></span>
<span><span class="co">#&gt; [149] 2 1</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="hierarchical-clustering">Hierarchical Clustering<a class="anchor" aria-label="anchor" href="#hierarchical-clustering"></a>
</h3>
<p>Hierarchical clustering is also a common clustering algorithm,
available in base R through the <code><a href="https://rdrr.io/r/stats/hclust.html" class="external-link">hclust()</a></code> function. This
method is a bit different in the sense that is does not straight up
return clusters. Instead, in creates a hierarchical structure (a
<em>dendrogram</em>), a tree from which we can <em>cut</em> branches to
get a given number of clusters. Note that this “tree” cutting can be
done in an unsupervised fashion too using bootstrapping (which we will
apply in the next section).</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">rez_hclust</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cluster_analysis.html">cluster_analysis</a></span><span class="op">(</span><span class="va">data</span>, n <span class="op">=</span> <span class="fl">3</span>, method <span class="op">=</span> <span class="st">"hclust"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">rez_hclust</span> <span class="co"># Show results</span></span>
<span><span class="co">#&gt; # Clustering Solution</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; The 3 clusters accounted for 74.35% of the total variance of the original data.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Cluster | n_Obs | Sum_Squares | Sepal.Length | Sepal.Width | Petal.Length | Petal.Width</span></span>
<span><span class="co">#&gt; ---------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 1       |    49 |       40.12 |        -1.00 |        0.90 |        -1.30 |       -1.25</span></span>
<span><span class="co">#&gt; 2       |    24 |       18.65 |        -0.40 |       -1.36 |         0.06 |       -0.04</span></span>
<span><span class="co">#&gt; 3       |    77 |       94.08 |         0.76 |       -0.15 |         0.81 |        0.81</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; # Indices of model performance</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Sum_Squares_Total | Sum_Squares_Between | Sum_Squares_Within |    R2</span></span>
<span><span class="co">#&gt; --------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 596.000           |             443.143 |            152.857 | 0.744</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; # You can access the predicted clusters via `predict()`.</span></span>
<span></span>
<span><span class="co"># Visualize</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">rez_hclust</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://easystats.github.io/see/reference/theme_modern.html" class="external-link">theme_modern</a></span><span class="op">(</span><span class="op">)</span> <span class="co"># Visualize</span></span></code></pre></div>
<p><img src="clustering_files/figure-html/unnamed-chunk-8-1.png" width="700"></p>
</div>
<div class="section level3">
<h3 id="hierarchical-k-means">Hierarchical K-Means<a class="anchor" aria-label="anchor" href="#hierarchical-k-means"></a>
</h3>
<p>Hierarchical K-Means, as its name suggest, is essentially a
combination of K-Means and hierarchical clustering that aims at
improving the stability and robustness of the results.</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">rez_hkmeans</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cluster_analysis.html">cluster_analysis</a></span><span class="op">(</span><span class="va">data</span>, n <span class="op">=</span> <span class="fl">3</span>, method <span class="op">=</span> <span class="st">"hkmeans"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">rez_hkmeans</span> <span class="co"># Show results</span></span>
<span><span class="co">#&gt; # Clustering Solution</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; The 3 clusters accounted for 76.70% of the total variance of the original data.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Cluster | n_Obs | Sum_Squares | Sepal.Length | Sepal.Width | Petal.Length | Petal.Width</span></span>
<span><span class="co">#&gt; ---------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 1       |    50 |       47.35 |        -1.01 |        0.85 |        -1.30 |       -1.25</span></span>
<span><span class="co">#&gt; 2       |    53 |       44.09 |        -0.05 |       -0.88 |         0.35 |        0.28</span></span>
<span><span class="co">#&gt; 3       |    47 |       47.45 |         1.13 |        0.09 |         0.99 |        1.01</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; # Indices of model performance</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Sum_Squares_Total | Sum_Squares_Between | Sum_Squares_Within |    R2</span></span>
<span><span class="co">#&gt; --------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 596.000           |             457.112 |            138.888 | 0.767</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; # You can access the predicted clusters via `predict()`.</span></span>
<span></span>
<span><span class="co"># Visualize</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">rez_hkmeans</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://easystats.github.io/see/reference/theme_modern.html" class="external-link">theme_modern</a></span><span class="op">(</span><span class="op">)</span> <span class="co"># Visualize</span></span></code></pre></div>
<p><img src="clustering_files/figure-html/unnamed-chunk-9-1.png" width="700"></p>
</div>
<div class="section level3">
<h3 id="k-medoids-pam">K-Medoids (PAM)<a class="anchor" aria-label="anchor" href="#k-medoids-pam"></a>
</h3>
<p>Clustering around “medoids”, instead of “centroid”, is considered to
be a more robust version of K-means. See <code><a href="https://rdrr.io/pkg/cluster/man/pam.html" class="external-link">cluster::pam()</a></code> for
more information.</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">rez_pam</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cluster_analysis.html">cluster_analysis</a></span><span class="op">(</span><span class="va">data</span>, n <span class="op">=</span> <span class="fl">3</span>, method <span class="op">=</span> <span class="st">"pam"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">rez_pam</span> <span class="co"># Show results</span></span>
<span><span class="co">#&gt; # Clustering Solution</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; The 3 clusters accounted for 76.46% of the total variance of the original data.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Cluster | n_Obs | Sum_Squares | Sepal.Length | Sepal.Width | Petal.Length | Petal.Width</span></span>
<span><span class="co">#&gt; ---------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 1       |    50 |       47.35 |        -1.01 |        0.85 |        -1.30 |       -1.25</span></span>
<span><span class="co">#&gt; 2       |    45 |       45.26 |         1.17 |        0.06 |         1.02 |        1.05</span></span>
<span><span class="co">#&gt; 3       |    55 |       47.67 |        -0.04 |       -0.82 |         0.35 |        0.28</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; # Indices of model performance</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Sum_Squares_Total | Sum_Squares_Between | Sum_Squares_Within |    R2</span></span>
<span><span class="co">#&gt; --------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 596.000           |             455.714 |            140.286 | 0.765</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; # You can access the predicted clusters via `predict()`.</span></span>
<span></span>
<span><span class="co"># Visualize</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">rez_pam</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://easystats.github.io/see/reference/theme_modern.html" class="external-link">theme_modern</a></span><span class="op">(</span><span class="op">)</span> <span class="co"># Visualize</span></span></code></pre></div>
<p><img src="clustering_files/figure-html/unnamed-chunk-10-1.png" width="700"></p>
</div>
</div>
<div class="section level2">
<h2 id="unsupervised-clustering-methods">Unsupervised Clustering Methods<a class="anchor" aria-label="anchor" href="#unsupervised-clustering-methods"></a>
</h2>
<p>Unsupervised clustering methods estimate the optimal number of
clusters themselves (hence, <code>n = NULL</code> as we don’t
pre-specify a given number of clusters). Note that unsupervised methods
can sometimes identify observations that do not fit under any clusters
(i.e., <strong>“outliers”</strong>). They will be classified as
belonging to the cluster “0” (which is not a real cluster, but rather
groups all the outliers).</p>
<div class="section level3">
<h3 id="bootstrapped-hierarchical-clustering">Bootstrapped Hierarchical Clustering<a class="anchor" aria-label="anchor" href="#bootstrapped-hierarchical-clustering"></a>
</h3>
<p>This method computes p-values for each cluster of the hierarchical
cluster structure, and returns the <strong>significant</strong>
clusters. This method can return a larger number of smaller clusters
and, because it’s based on bootstrapping, is quite slow.</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">rez_hclust2</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cluster_analysis.html">cluster_analysis</a></span><span class="op">(</span><span class="va">data</span>,</span>
<span>  n <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  method <span class="op">=</span> <span class="st">"hclust"</span>,</span>
<span>  iterations <span class="op">=</span> <span class="fl">500</span>,</span>
<span>  ci <span class="op">=</span> <span class="fl">0.90</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">rez_hclust2</span> <span class="co"># Show results</span></span>
<span><span class="co">#&gt; # Clustering Solution</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; The 25 clusters accounted for 48.37% of the total variance of the original data.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Cluster | n_Obs | Sum_Squares | Sepal.Length | Sepal.Width | Petal.Length | Petal.Width</span></span>
<span><span class="co">#&gt; ---------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 0       |    89 |      304.31 |         0.11 |       -0.19 |         0.12 |        0.12</span></span>
<span><span class="co">#&gt; 1       |     2 |    7.29e-03 |        -0.96 |        0.79 |        -1.28 |       -1.31</span></span>
<span><span class="co">#&gt; 10      |     2 |        0.02 |        -0.23 |       -0.13 |         0.22 |        0.07</span></span>
<span><span class="co">#&gt; 11      |     2 |        0.02 |         0.49 |        0.79 |         0.99 |        1.51</span></span>
<span><span class="co">#&gt; 12      |     2 |        0.03 |        -0.41 |       -0.13 |         0.42 |        0.39</span></span>
<span><span class="co">#&gt; 13      |     2 |        0.03 |        -1.02 |        0.44 |        -1.39 |       -1.31</span></span>
<span><span class="co">#&gt; 14      |     2 |        0.03 |        -1.08 |       -1.62 |        -0.26 |       -0.26</span></span>
<span><span class="co">#&gt; 15      |     3 |        0.07 |        -1.78 |       -0.21 |        -1.41 |       -1.35</span></span>
<span><span class="co">#&gt; 16      |     3 |        0.09 |        -0.13 |       -0.74 |         0.72 |        0.96</span></span>
<span><span class="co">#&gt; 17      |     3 |        0.12 |        -0.50 |        0.86 |        -1.28 |       -1.22</span></span>
<span><span class="co">#&gt; 18      |     3 |        0.09 |        -1.34 |        0.79 |        -1.20 |       -1.27</span></span>
<span><span class="co">#&gt; 19      |     2 |        0.08 |         2.18 |       -0.13 |         1.47 |        1.31</span></span>
<span><span class="co">#&gt; 2       |     2 |    7.29e-03 |        -0.60 |        1.47 |        -1.28 |       -1.31</span></span>
<span><span class="co">#&gt; 20      |     2 |        0.10 |        -0.60 |        2.51 |        -1.31 |       -1.38</span></span>
<span><span class="co">#&gt; 21      |     2 |        0.15 |         1.64 |        0.10 |         1.21 |        0.66</span></span>
<span><span class="co">#&gt; 22      |     3 |        0.22 |         0.39 |       -1.89 |         0.50 |        0.31</span></span>
<span><span class="co">#&gt; 23      |     7 |        1.42 |         0.29 |        0.23 |         0.57 |        0.66</span></span>
<span><span class="co">#&gt; 24      |     3 |        0.80 |         2.12 |        1.55 |         1.50 |        1.36</span></span>
<span><span class="co">#&gt; 3       |     2 |    8.61e-03 |         0.67 |       -0.59 |         1.04 |        1.25</span></span>
<span><span class="co">#&gt; 4       |     2 |        0.01 |        -0.41 |       -1.51 |    -4.53e-03 |       -0.20</span></span>
<span><span class="co">#&gt; 5       |     2 |        0.01 |        -0.90 |        1.70 |        -1.25 |       -1.25</span></span>
<span><span class="co">#&gt; 6       |     2 |        0.01 |         1.22 |        0.33 |         1.16 |        1.44</span></span>
<span><span class="co">#&gt; 7       |     2 |        0.02 |        -1.08 |        1.25 |        -1.34 |       -1.38</span></span>
<span><span class="co">#&gt; 8       |     3 |        0.02 |        -0.94 |        1.02 |        -1.35 |       -1.22</span></span>
<span><span class="co">#&gt; 9       |     3 |        0.02 |        -1.18 |        0.10 |        -1.26 |       -1.35</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; # Indices of model performance</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Sum_Squares_Total | Sum_Squares_Between | Sum_Squares_Within |    R2</span></span>
<span><span class="co">#&gt; --------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 596.000           |             288.295 |              3.390 | 0.484</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; # You can access the predicted clusters via `predict()`.</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">rez_hclust2</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://easystats.github.io/see/reference/theme_modern.html" class="external-link">theme_modern</a></span><span class="op">(</span><span class="op">)</span> <span class="co"># Visualize</span></span></code></pre></div>
<p><img src="clustering_files/figure-html/unnamed-chunk-11-1.png" width="700"></p>
</div>
<div class="section level3">
<h3 id="dbscan">DBSCAN<a class="anchor" aria-label="anchor" href="#dbscan"></a>
</h3>
<p>Although the DBSCAN method is quite powerful to identify clusters, it
is highly dependent on its parameters, namely, <code>eps</code> and the
<code>min_size</code>. Regarding the latter, the minimum size of any
cluster is set by default to <code>0.1</code> (i.e., 10% of rows), which
is appropriate to avoid having too small clusters.</p>
<p>The “optimal” <strong>eps</strong> value can be estimated using the
<a href="https://easystats.github.io/parameters/reference/cluster_analysis.html"><code>n_clusters_dbscan()</code></a>
function:</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">eps</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/n_clusters.html">n_clusters_dbscan</a></span><span class="op">(</span><span class="va">data</span>, min_size <span class="op">=</span> <span class="fl">0.1</span><span class="op">)</span></span>
<span><span class="va">eps</span></span>
<span><span class="co">#&gt; The DBSCAN method, based on the total clusters sum of squares, suggests that the optimal eps = 2.11193281281293 (with min. cluster size set to 15), which corresponds to 1 clusters.</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">eps</span><span class="op">)</span></span></code></pre></div>
<p><img src="clustering_files/figure-html/unnamed-chunk-12-1.png" width="700"></p>
<p>It seems like the numeric method to find the elbow of the curve
doesn’t work well, and returns a value that is too high. Based on visual
assessment, the elbow seems to be located around
<code>eps = 1.45</code>.</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">rez_dbscan</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cluster_analysis.html">cluster_analysis</a></span><span class="op">(</span><span class="va">data</span>, method <span class="op">=</span> <span class="st">"dbscan"</span>, dbscan_eps <span class="op">=</span> <span class="fl">1.45</span><span class="op">)</span></span>
<span></span>
<span><span class="va">rez_dbscan</span> <span class="co"># Show results</span></span>
<span><span class="co">#&gt; # Clustering Solution</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; The 3 clusters accounted for 61.14% of the total variance of the original data.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Cluster | n_Obs | Sum_Squares | Sepal.Length | Sepal.Width | Petal.Length | Petal.Width</span></span>
<span><span class="co">#&gt; ---------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 0       |     5 |       47.84 |         1.03 |        0.74 |         0.45 |        0.32</span></span>
<span><span class="co">#&gt; 1       |    48 |       34.54 |        -1.02 |        0.86 |        -1.30 |       -1.26</span></span>
<span><span class="co">#&gt; 2       |    97 |      149.21 |         0.45 |       -0.46 |         0.62 |        0.61</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; # Indices of model performance</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Sum_Squares_Total | Sum_Squares_Between | Sum_Squares_Within |    R2</span></span>
<span><span class="co">#&gt; --------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 596.000           |             364.406 |            183.751 | 0.611</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; # You can access the predicted clusters via `predict()`.</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">rez_dbscan</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://easystats.github.io/see/reference/theme_modern.html" class="external-link">theme_modern</a></span><span class="op">(</span><span class="op">)</span> <span class="co"># Visualize</span></span></code></pre></div>
<p><img src="clustering_files/figure-html/unnamed-chunk-13-1.png" width="700"></p>
</div>
<div class="section level3">
<h3 id="hierarchical-k-means-1">Hierarchical K-Means<a class="anchor" aria-label="anchor" href="#hierarchical-k-means-1"></a>
</h3>
<p>Hierarchical DBSCAN is a variant that does not require the critical
<strong>EPS</strong> argument. It computes the hierarchy of all DBSCAN
solutions, and then finds the optimal cuts in the hierarchy using a
stability-based extraction method.</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">rez_hdbscan</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cluster_analysis.html">cluster_analysis</a></span><span class="op">(</span><span class="va">data</span>, method <span class="op">=</span> <span class="st">"hdbscan"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">rez_hdbscan</span> <span class="co"># Show results</span></span>
<span><span class="co">#&gt; # Clustering Solution</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; The 3 clusters accounted for 66.08% of the total variance of the original data.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Cluster | n_Obs | Sum_Squares | Sepal.Length | Sepal.Width | Petal.Length | Petal.Width</span></span>
<span><span class="co">#&gt; ---------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 0       |     2 |        0.08 |         2.36 |        1.70 |         1.58 |        1.18</span></span>
<span><span class="co">#&gt; 1       |    98 |      154.76 |         0.47 |       -0.47 |         0.63 |        0.61</span></span>
<span><span class="co">#&gt; 2       |    50 |       47.35 |        -1.01 |        0.85 |        -1.30 |       -1.25</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; # Indices of model performance</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Sum_Squares_Total | Sum_Squares_Between | Sum_Squares_Within |    R2</span></span>
<span><span class="co">#&gt; --------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 596.000           |             393.813 |            202.108 | 0.661</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; # You can access the predicted clusters via `predict()`.</span></span>
<span></span>
<span><span class="co"># Visualize</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">rez_hdbscan</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://easystats.github.io/see/reference/theme_modern.html" class="external-link">theme_modern</a></span><span class="op">(</span><span class="op">)</span> <span class="co"># Visualize</span></span></code></pre></div>
<p><img src="clustering_files/figure-html/unnamed-chunk-14-1.png" width="700"></p>
</div>
<div class="section level3">
<h3 id="k-medoids-with-estimation-of-number-of-clusters-pamk">K-Medoids with estimation of number of clusters (pamk)<a class="anchor" aria-label="anchor" href="#k-medoids-with-estimation-of-number-of-clusters-pamk"></a>
</h3>
<p>This is K-Medoids with an integrated estimation of the number of
clusters. See <code><a href="https://rdrr.io/pkg/fpc/man/pamk.html" class="external-link">fpc::pamk</a></code> for more details.</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">rez_pamk</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cluster_analysis.html">cluster_analysis</a></span><span class="op">(</span><span class="va">data</span>, method <span class="op">=</span> <span class="st">"pamk"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">rez_pamk</span> <span class="co"># Show results</span></span>
<span><span class="co">#&gt; # Clustering Solution</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; The 2 clusters accounted for 62.94% of the total variance of the original data.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Cluster | n_Obs | Sum_Squares | Sepal.Length | Sepal.Width | Petal.Length | Petal.Width</span></span>
<span><span class="co">#&gt; ---------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 1       |    50 |       47.35 |        -1.01 |        0.85 |        -1.30 |       -1.25</span></span>
<span><span class="co">#&gt; 2       |   100 |      173.53 |         0.51 |       -0.43 |         0.65 |        0.63</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; # Indices of model performance</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Sum_Squares_Total | Sum_Squares_Between | Sum_Squares_Within |    R2</span></span>
<span><span class="co">#&gt; --------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 596.000           |             375.121 |            220.879 | 0.629</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; # You can access the predicted clusters via `predict()`.</span></span>
<span></span>
<span><span class="co"># Visualize</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">rez_pamk</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://easystats.github.io/see/reference/theme_modern.html" class="external-link">theme_modern</a></span><span class="op">(</span><span class="op">)</span> <span class="co"># Visualize</span></span></code></pre></div>
<p><img src="clustering_files/figure-html/unnamed-chunk-15-1.png" width="700"></p>
</div>
<div class="section level3">
<h3 id="mixture">Mixture<a class="anchor" aria-label="anchor" href="#mixture"></a>
</h3>
<p>Model-based clustering based on finite Gaussian mixture models.
Models are estimated by EM algorithm initialized by hierarchical
model-based agglomerative clustering. The optimal model is then selected
according to BIC.</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://mclust-org.github.io/mclust/" class="external-link">mclust</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">rez_mixture</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cluster_analysis.html">cluster_analysis</a></span><span class="op">(</span><span class="va">data</span>, method <span class="op">=</span> <span class="st">"mixture"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">rez_mixture</span> <span class="co"># Show results</span></span>
<span><span class="co">#&gt; # Clustering Solution</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; The 2 clusters accounted for 62.94% of the total variance of the original data.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Cluster | n_Obs | Sum_Squares | Sepal.Length | Sepal.Width | Petal.Length | Petal.Width</span></span>
<span><span class="co">#&gt; ---------------------------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 1       |    50 |       47.35 |        -1.01 |        0.85 |        -1.30 |       -1.25</span></span>
<span><span class="co">#&gt; 2       |   100 |      173.53 |         0.51 |       -0.43 |         0.65 |        0.63</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; # Indices of model performance</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Sum_Squares_Total | Sum_Squares_Between | Sum_Squares_Within |    R2</span></span>
<span><span class="co">#&gt; --------------------------------------------------------------------</span></span>
<span><span class="co">#&gt; 596.000           |             375.121 |            220.879 | 0.629</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; # You can access the predicted clusters via `predict()`.</span></span>
<span></span>
<span><span class="co"># Visualize</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">rez_mixture</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://easystats.github.io/see/reference/theme_modern.html" class="external-link">theme_modern</a></span><span class="op">(</span><span class="op">)</span> <span class="co"># Visualize</span></span></code></pre></div>
<p><img src="clustering_files/figure-html/unnamed-chunk-16-1.png" width="700"></p>
</div>
</div>
<div class="section level2">
<h2 id="metaclustering">Metaclustering<a class="anchor" aria-label="anchor" href="#metaclustering"></a>
</h2>
<p>One of the core “issue” of statistical clustering is that, in many
cases, different methods will give different results. The
<strong>metaclustering</strong> approach proposed by <em>easystats</em>
(that finds echoes in <em>consensus clustering</em>; see Monti et al.,
2003) consists of treating the unique clustering solutions as a
ensemble, from which we can derive a probability matrix. This matrix
contains, for each pair of observations, the probability of being in the
same cluster. For instance, if the 6th and the 9th row of a dataframe
has been assigned to a similar cluster by 5 our of 10 clustering
methods, then its probability of being grouped together is 0.5.</p>
<p>Metaclustering is based on the hypothesis that, as each clustering
algorithm embodies a different prism by which it sees the data, running
an infinite amount of algorithms would result in the emergence of the
“true” clusters. As the number of algorithms and parameters is finite,
the probabilistic perspective is a useful proxy. This method is
interesting where there is no obvious reasons to prefer one over another
clustering method, as well as to investigate how robust some clusters
are under different algorithms.</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">list_of_results</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span></span>
<span>  <span class="va">rez_kmeans</span>, <span class="va">rez_hclust</span>, <span class="va">rez_hkmeans</span>, <span class="va">rez_pam</span>,</span>
<span>  <span class="va">rez_hclust2</span>, <span class="va">rez_dbscan</span>, <span class="va">rez_hdbscan</span>, <span class="va">rez_mixture</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">probability_matrix</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cluster_meta.html">cluster_meta</a></span><span class="op">(</span><span class="va">list_of_results</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Plot the matrix as a reordered heatmap</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/heatmap.html" class="external-link">heatmap</a></span><span class="op">(</span><span class="va">probability_matrix</span>,</span>
<span>  scale <span class="op">=</span> <span class="st">"none"</span>,</span>
<span>  col <span class="op">=</span> <span class="fu">grDevices</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/r/grDevices/palettes.html" class="external-link">hcl.colors</a></span><span class="op">(</span><span class="fl">256</span>, palette <span class="op">=</span> <span class="st">"inferno"</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p><img src="clustering_files/figure-html/unnamed-chunk-17-1.png" width="700"></p>
<p>The dendrogram (which is a <strong>hierarchical clustering of the
clustering solution</strong>, hence the name of
<strong>meta</strong>clustering), as well as the heatmap (in which the
darker squares represent a higher probability of belonging to the same
cluster) shows that there is one metacluster consisting of the 1-50
first rows (bottom left), and then the rest of the observations are
closer to one another. However, two subclusters are still visible,
corresponding to the “true” species.</p>
<p>The metaclustering approach confirms our initial hypothesis, <em>the
<strong>setosa</strong> species stands out quite clearly, and the
separation between the two other species is less clear cut</em>.</p>
</div>
<div class="section level2">
<h2 id="resources">Resources<a class="anchor" aria-label="anchor" href="#resources"></a>
</h2>
<ul>
<li><a href="https://scikit-learn.org/stable/modules/clustering.html" class="external-link">Clustering
algorithms overview</a></li>
<li><a href="https://www.datanovia.com/en/lessons/dbscan-density-based-clustering-essentials/" class="external-link">Density-based
Clustering</a></li>
</ul>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by <a href="https://github.com/strengejacke" class="external-link">Daniel Lüdecke</a>, <a href="https://dominiquemakowski.github.io/" class="external-link">Dominique Makowski</a>, <a href="https://home.msbstats.info/" class="external-link">Mattan S. Ben-Shachar</a>, <a href="https://sites.google.com/site/indrajeetspatilmorality/" class="external-link">Indrajeet Patil</a>, Søren Højsgaard, <a href="https://wiernik.org/" class="external-link">Brenton M. Wiernik</a>.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.9000.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
